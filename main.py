import sys
import requests
import json
import os
from PyQt5.QtWidgets import QApplication
from ui.chat_ui import ChatUI

# ä»configè°ƒå–é…ç½®
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
with open(os.path.join(BASE_DIR, "config.json"), "r", encoding="utf-8") as f:
    configjson = json.load(f)

OLLAMA_API_URL = configjson["api"]["local_api"]
MODEL = configjson["api"]["local_model"]
AI_NAME = configjson["general"]["AI_name"]
print(OLLAMA_API_URL, MODEL, AI_NAME)

# è¯»å– system æç¤ºè¯ï¼ˆsystem/prompts/personality.txtï¼‰
SYSTEM_PROMPT = None
_personality_path = os.path.join(BASE_DIR, "system", "prompts", "personality.txt")
if os.path.exists(_personality_path):
    try:
        with open(_personality_path, "r", encoding="utf-8") as f:
            SYSTEM_PROMPT = f.read().strip()https://github.com/Starsky227/LingYiProject.git
    except Exception as e:
        print(f"[è­¦å‘Š] æ— æ³•è¯»å–ç³»ç»Ÿæç¤ºè¯: {_personality_path} - {e}")
else:
    print(f"[æç¤º] æœªæ‰¾åˆ°ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶: {_personality_path}")

# =============== æ¨¡å‹äº¤äº’éƒ¨åˆ† ===============
def chat_with_model(messages, on_response):
    """
    å‘é€æ¶ˆæ¯ç»™ Ollama å¹¶æµå¼è¾“å‡ºå›å¤ï¼ˆè°ƒç”¨ on_response è¿›è¡Œæµå¼æ˜¾ç¤ºï¼‰
    """
    try:
        # åœ¨å‘é€å‰å°† system æç¤ºè¯æ’å…¥åˆ°æ¶ˆæ¯å¼€å¤´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        payload_messages = []
        if SYSTEM_PROMPT:
            payload_messages.append({"role": "system", "content": SYSTEM_PROMPT})
        # è¿½åŠ å½“å‰å¯¹è¯æ¶ˆæ¯ï¼ˆä¸ä¿®æ”¹åŸ messages å˜é‡ï¼‰
        payload_messages.extend(messages)

        response = requests.post(
            OLLAMA_API_URL,
            json={"model": MODEL, "messages": payload_messages},
            stream=True,
        )

        full_reply = ""
        for line in response.iter_lines():
            if line:
                data = json.loads(line.decode("utf-8"))
                # å¦‚æœå½“å‰è¡ŒåŒ…å«æ¶ˆæ¯å†…å®¹ï¼Œåˆ™æå–å¹¶é€šè¿‡å›è°ƒå‘é€ç»™ UI
                if "message" in data and "content" in data["message"]:
                    content = data["message"]["content"]
                    full_reply += content
                    # é€šçŸ¥ UI æ”¶åˆ°ä¸€æ®µæµå¼è¾“å‡º
                    on_response(content)

        return full_reply
    except Exception as e:
        # å‡ºç°å¼‚å¸¸æ—¶é€šè¿‡å›è°ƒå‘é€é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äº UI å±•ç¤º
        on_response(f"\n[é”™è¯¯] æ— æ³•è¿æ¥åˆ° Ollama: {e}\n")
        return ""


# =============== å¯åŠ¨å™¨éƒ¨åˆ† ===============
def main():
    app = QApplication(sys.argv)
    # å°† chat_with_model å‡½æ•°ä¸ AI åä¼ å…¥ UI ç•Œé¢
    window = ChatUI(chat_with_model, ai_name=AI_NAME)
    window.show()
    sys.exit(app.exec_())

if __name__ == "__main__":
    print("ğŸ§  å¯åŠ¨æœ¬åœ° Gemma3 èŠå¤©ç¨‹åºä¸­...")
    main()
